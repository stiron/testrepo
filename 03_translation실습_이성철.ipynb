{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 실습\n",
        "\n",
        "1. 주어진 데이터를 활용하여 encoder-decoder 구조 번역모델을 학습하기\n",
        "2. eng-fra.txt와 주어진 데이터의 차이점이 있다면 데이터 형식 맞춰서 진행하기"
      ],
      "metadata": {
        "id": "_iY7_vY7BdKp"
      },
      "id": "_iY7_vY7BdKp"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qipVTjs2f0n7",
        "outputId": "86857d86-6743-41a6-f1a4-2ce76f8fbeb6"
      },
      "id": "qipVTjs2f0n7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ae5b052",
      "metadata": {
        "id": "6ae5b052"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c4841b42",
      "metadata": {
        "id": "c4841b42"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  \n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(df, lang):\n",
        "    sentence = df[lang].str.lower()\n",
        "    sentence = sentence.str.replace('[^A-Za-z\\s]+', '')\n",
        "    sentence = sentence.str.normalize('NFD')\n",
        "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "jTflBum73vbH"
      },
      "id": "jTflBum73vbH",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5B1dxYv94Zu4"
      },
      "id": "5B1dxYv94Zu4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "58ebe3bd",
      "metadata": {
        "id": "58ebe3bd"
      },
      "outputs": [],
      "source": [
        "def read_sentence(df, lang1, lang2):\n",
        "    sentence1 = normalizeString(df, lang1)\n",
        "    sentence2 = normalizeString(df, lang2)\n",
        "    return sentence1, sentence2\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
        "    return df\n",
        "\n",
        "def process_data(file_name, lang1,lang2):\n",
        "    df = read_file(file_name, lang1, lang2) #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n",
        "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "\n",
        "    input_lang = Lang()\n",
        "    output_lang = Lang()\n",
        "    pairs = []\n",
        "    for i in range(len(df)):\n",
        "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
        "            full = [sentence1[i], sentence2[i]]\n",
        "            input_lang.addSentence(sentence1[i])\n",
        "            output_lang.addSentence(sentence2[i])\n",
        "            pairs.append(full)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f0c3e7a2",
      "metadata": {
        "id": "f0c3e7a2"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e5cda608",
      "metadata": {
        "id": "e5cda608"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
        "        super(Encoder, self).__init__()       \n",
        "        self.input_dim = input_dim\n",
        "        self.embbed_dim = embbed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "    def forward(self, src):      \n",
        "        embedded = self.embedding(src).view(1,1,-1)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8c8c263c",
      "metadata": {
        "id": "8c8c263c"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embbed_dim = embbed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)    \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.view(1, -1)\n",
        "        embedded = F.relu(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)       \n",
        "        prediction = self.softmax(self.out(output[0]))      \n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2e32442e",
      "metadata": {
        "id": "2e32442e"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "      \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "     \n",
        "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
        "        input_length = input_lang.size(0) #소스언어의 Vocab 갯수\n",
        "        batch_size = output_lang.shape[1] \n",
        "        target_length = output_lang.shape[0]\n",
        "        vocab_size = self.decoder.output_dim      \n",
        "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n",
        "\n",
        "        decoder_hidden = encoder_hidden.to(device)  \n",
        "        decoder_input = torch.tensor([SOS_token], device=device)  \n",
        "\n",
        "        for t in range(target_length):   \n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs[t] = decoder_output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            input = (output_lang[t] if teacher_force else topi)\n",
        "            if(teacher_force == False and input.item() == EOS_token):\n",
        "                break\n",
        "        return outputs\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "93d4ebc3",
      "metadata": {
        "id": "93d4ebc3"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "    model_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "    loss = 0\n",
        "    epoch_loss = 0\n",
        "    output = model(input_tensor, target_tensor)\n",
        "    num_iter = output.size(0)\n",
        "\n",
        "    for ot in range(num_iter):\n",
        "        loss += criterion(output[ot], target_tensor[ot])\n",
        "\n",
        "    loss.backward()\n",
        "    model_optimizer.step()\n",
        "    epoch_loss = loss.item() / num_iter\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9768b4e8",
      "metadata": {
        "id": "9768b4e8"
      },
      "outputs": [],
      "source": [
        "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.NLLLoss()\n",
        "    total_loss_iterations = 0\n",
        "\n",
        "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
        "                      for i in range(num_iteration)]\n",
        "  \n",
        "    for iter in range(1, num_iteration+1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "        total_loss_iterations += loss\n",
        "\n",
        "        if iter % 5000 == 0:\n",
        "            avarage_loss= total_loss_iterations / 5000\n",
        "            total_loss_iterations = 0\n",
        "            print('%d %.4f' % (iter, avarage_loss))\n",
        "          \n",
        "    torch.save(model.state_dict(), f'mytraining.pt') \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7c1c53aa",
      "metadata": {
        "id": "7c1c53aa"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
        "        output_tensor = tensorFromSentence(output_lang, sentences[1])  \n",
        "        decoded_words = []  \n",
        "        output = model(input_tensor, output_tensor)\n",
        "  \n",
        "        for ot in range(output.size(0)):\n",
        "            topv, topi = output[ot].topk(1)\n",
        "\n",
        "            if topi[0].item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('input {}'.format(pair[0]))\n",
        "        print('output {}'.format(pair[1]))\n",
        "        output_words = evaluate(model, input_lang, output_lang, pair)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('predicted {}'.format(output_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang1 = 'kor'\n",
        "lang2 = 'eng'\n",
        "input_lang, output_lang, pairs = process_data(\"kor_eng_train.txt\",lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "input_size = input_lang.n_words\n",
        "output_size = output_lang.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "#num_iteration = 75000\n",
        "num_iteration = 10000\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        " \n",
        "print(encoder)\n",
        "print(decoder)\n",
        "print(model)\n",
        "model = trainModel(model, input_lang, output_lang, pairs, num_iteration)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Gn4u8KTZ7r",
        "outputId": "70fc4e3c-8c21-4e2c-8496-4ab243d29f51"
      },
      "id": "n4Gn4u8KTZ7r",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random sentence ['              ', 'aboard were  migrants from subsaharan africa and a dead woman']\n",
            "Input : 189 Output : 3151\n",
            "Encoder(\n",
            "  (embedding): Embedding(189, 256)\n",
            "  (gru): GRU(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(3151, 256)\n",
            "  (gru): GRU(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=3151, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(189, 256)\n",
            "    (gru): GRU(256, 512)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(3151, 256)\n",
            "    (gru): GRU(256, 512)\n",
            "    (out): Linear(in_features=512, out_features=3151, bias=True)\n",
            "    (softmax): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n",
            "5000 4.8519\n",
            "10000 4.7082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = process_data(\"kor_eng_test.txt\",lang1, lang2)\n",
        "evaluateRandomly(model, input_lang, output_lang, pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JWe0pQjTyEz",
        "outputId": "671349fa-43fb-4b58-98f7-55c4e616f7e4"
      },
      "id": "8JWe0pQjTyEz",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input       \n",
            "output kalmaegi the korean word for sea gull was the first storm to hit taiwan this year\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input               \n",
            "output negotiations for their release were hampered by ransom demands and reports of heavy beatings by their muslim captors\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input           \n",
            "output even though the threat is small the potential effects are devastating\n",
            "predicted the the the  in in in the the to to in\n",
            "input           \n",
            "output south korea has a better opportunity to make it to the roundof than in any previous world cup\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input                 \n",
            "output nouri maliki told reuters news agency there was a limit to the acceptable excuses for civilian casualties\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input                \n",
            "output he is expected to leave the academic medical center on tuesday hospital spokesman johan kortenray said\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input dissident     \n",
            "output amnesty is celebrating  years of activism by highlighting governments using the net to suppress dissent\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input        \n",
            "output however it turned out that i and my administration were isolated\n",
            "predicted the the the  in in in the the to to in\n",
            "input               \n",
            "output that result would be a blow to hillary clinton who stood her best chance in virginia\n",
            "predicted the the the  in in in the the to to in <EOS>\n",
            "input       \n",
            "output authorities said they used dental records to confirm johnsons identity\n",
            "predicted the the the  in in in the the to to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jKrof6EWTzbz"
      },
      "id": "jKrof6EWTzbz",
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "03-translation실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}